{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eBaKsoPeoykt"
   },
   "outputs": [],
   "source": [
    "# Developer docs\n",
    "# https://www.tensorflow.org/tutorials/keras/classification\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "\n",
    "# Importing modules\n",
    "from os import listdir, system\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import cv2 as cv2\n",
    "import imutils\n",
    "import imageio\n",
    "import pickle\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import tensorflow as tf\n",
    "from skimage.io import imread\n",
    "from skimage import measure\n",
    "from skimage.measure import regionprops\n",
    "from skimage.transform import resize\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "# Downloading dataset from: http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/\n",
    "# You can comment this after running once.\n",
    "\n",
    "# !wget http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/EnglishImg.tgz\n",
    "# !tar -xvzf ./EnglishImg.tgz && rm -rf EnglishImg.tgz\n",
    "# !git clone https://github.com/captcha-breakers/dataset-simple-uppercase.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IIDQCtPpAxe",
    "outputId": "b4e51a26-7c79-43d9-f497-7c018c14171b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sample011', 'Sample012', 'Sample013', 'Sample014', 'Sample015', 'Sample016', 'Sample017', 'Sample018', 'Sample019', 'Sample020', 'Sample021', 'Sample022', 'Sample023', 'Sample024', 'Sample025', 'Sample026', 'Sample027', 'Sample028', 'Sample029', 'Sample030', 'Sample031', 'Sample032', 'Sample033', 'Sample034', 'Sample035', 'Sample036']\n"
     ]
    }
   ],
   "source": [
    "# Setting up folders\n",
    "base_dir = \"./English/Img/GoodImg/Bmp/\"\n",
    "folders = listdir(base_dir)\n",
    "folders.sort()\n",
    "folders = folders[10:36]\n",
    "print(folders)\n",
    "\n",
    "# Setting image_size\n",
    "image_size = (24, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dnjsqbFbpFOQ",
    "outputId": "2b963fb0-ddc0-40e4-d458-e612624080eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images:  3085\n"
     ]
    }
   ],
   "source": [
    "# Collecting images from folders\n",
    "data = []\n",
    "total_images = 0\n",
    "for i in folders:\n",
    "    all_images = listdir(base_dir+i)[:150]\n",
    "    total_images+=len(all_images)\n",
    "    currentlabel = int(i[6:])-11\n",
    "\n",
    "    for name in all_images:\n",
    "        image = cv2.imread(base_dir+i+\"/\"+name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image = cv2.resize(image, image_size)\n",
    "\n",
    "        # Filtering\n",
    "        image = cv2.GaussianBlur(image,(5,5),0)\n",
    "        image = cv2.Laplacian(image,cv2.CV_64F)\n",
    "        image = cv2.GaussianBlur(image,(5,5),0)\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        image = cv2.filter2D(image,-1,kernel)\n",
    "        \n",
    "        data.append((image/255, currentlabel))\n",
    "print(\"Total images: \", total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "UpjV_ma4613F",
    "outputId": "911b37cb-eabc-4ec4-b895-2e4470aea1a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd4b0f434c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing images\n",
    "imshow(data[6][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kBEvr3VRpKLz"
   },
   "outputs": [],
   "source": [
    "# Shuffling data & making train/test\n",
    "np.random.shuffle(data)\n",
    "n = len(data)\n",
    "p = int(80*n/100)\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for i in range(n):\n",
    "    if i<p:\n",
    "        train_images.append(data[i][0])\n",
    "        train_labels.append(data[i][1])\n",
    "    else:\n",
    "        test_images.append(data[i][0])\n",
    "        test_labels.append(data[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Yws9xKLpNHf",
    "outputId": "b2846e52-671f-4af1-c81a-2403996f574d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.1667 - accuracy: 0.1151\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.4253 - accuracy: 0.4278\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.5806 - accuracy: 0.6102\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.0482 - accuracy: 0.7358\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.7478 - accuracy: 0.8157\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5927 - accuracy: 0.8426\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.9089\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.9291\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2504 - accuracy: 0.9557\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1922 - accuracy: 0.9701\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1435 - accuracy: 0.9834\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1100 - accuracy: 0.9897\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0909 - accuracy: 0.9914\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0784 - accuracy: 0.9939\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0634 - accuracy: 0.9968\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0477 - accuracy: 0.9971\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0347 - accuracy: 0.9982\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9976\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 0.9991\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.9988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd4a04962b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the model and training it\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=image_size),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(26)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(np.array(train_images), np.array(train_labels), epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db79yJnOzy7g",
    "outputId": "598492f9-a611-4338-d5e8-33599d0e108b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  76.6612641815235\n"
     ]
    }
   ],
   "source": [
    "system(\"mkdir -p models\")\n",
    "# model.save('./models/my_model_84.h5')\n",
    "\n",
    "# Predicting on test images\n",
    "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "predictions = probability_model.predict(np.array(test_images))\n",
    "\n",
    "p=0\n",
    "for i in range(len(test_images)):\n",
    "  if np.argmax(predictions[i]) == test_labels[i]:\n",
    "    p+=1\n",
    "\n",
    "print(\"Test accuracy: \", 100*p/len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcEypgpaawjr",
    "outputId": "457c04b4-7a61-4861-cd01-1f62691c302f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Char</th>\n",
       "      <td>43548</td>\n",
       "      <td>13407</td>\n",
       "      <td>76.460363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Captcha</th>\n",
       "      <td>3152</td>\n",
       "      <td>6848</td>\n",
       "      <td>31.520000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current file:  10000\n",
      "FXYQIR : PXYQR\n"
     ]
    }
   ],
   "source": [
    "fil_dir = \"./dataset-simple-uppercase/data/\"\n",
    "total = 0\n",
    "char_p = 0\n",
    "char_n = 0\n",
    "cap_p = 0\n",
    "cap_n = 0\n",
    "\n",
    "for fil in listdir(fil_dir):\n",
    "  image = cv2.imread(fil_dir + fil)\n",
    "\n",
    "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n",
    "  _, thresh = cv2.threshold(gray, 120, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "  cnts, new = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "  cnts=sorted(cnts, key = cv2.contourArea, reverse = True)[:30] \n",
    "  captcha = np.invert(thresh)\n",
    "  labelled_captcha = measure.label(captcha)\n",
    "\n",
    "  character_dimensions = (0.20*captcha.shape[0], 0.8*captcha.shape[0], 0.03*captcha.shape[1], 0.25*captcha.shape[1])\n",
    "  min_height, max_height, min_width, max_width = character_dimensions\n",
    "\n",
    "  characters = []\n",
    "  charactersx = []\n",
    "  counter=0\n",
    "  column_list = []\n",
    "  row_list = []\n",
    "  d = []\n",
    "  for regions in regionprops(labelled_captcha):\n",
    "      y0, x0, y1, x1 = regions.bbox\n",
    "      region_height = y1 - y0\n",
    "      region_width = x1 - x0\n",
    "\n",
    "      if region_height > min_height and region_height < max_height and region_width > min_width and region_width < max_width:\n",
    "          roi = captcha[y0:y1, x0:x1]\n",
    "          resized_char = cv2.copyMakeBorder(roi, 10, 10, 10, 10, cv2.BORDER_CONSTANT)\n",
    "          resized_char = resize(roi, image_size)\n",
    "          column_list.append(x0)\n",
    "          d.append([x0, resized_char])\n",
    "  \n",
    "  sorted(d)\n",
    "  predicted_captcha = \"\"  \n",
    "  ind=0\n",
    "  for _,each_character in d:\n",
    "      x_p,y_p = 3,3\n",
    "      each_character = cv2.copyMakeBorder(each_character, x_p, x_p, y_p, y_p, cv2.BORDER_CONSTANT)\n",
    "      each_character = cv2.resize(each_character,image_size)\n",
    "#       imshow(each_character), plt.show()\n",
    "      each_character = each_character.reshape(1, -1)\n",
    "    \n",
    "      result, = probability_model.predict([each_character])\n",
    "      if fil[ind] == chr(65+np.argmax(result)):char_p += 1\n",
    "      else:char_n += 1\n",
    "      \n",
    "      predicted_captcha+=chr(65+np.argmax(result))\n",
    "      ind += 1\n",
    "  total+=1\n",
    "  \n",
    " \n",
    "  \n",
    "  if fil[:6] == predicted_captcha: cap_p+=1\n",
    "  else: cap_n+=1\n",
    "  \n",
    "  \"\"\"Statistics\"\"\"\n",
    "\n",
    "  stats = pd.DataFrame(\n",
    "    [\n",
    "        [char_p, char_n, 100*char_p/(char_p+char_n)], \n",
    "        [cap_p, cap_n, 100*cap_p/(cap_p+cap_n)]\n",
    "    ], \n",
    "    index=[\"Char\", \"Captcha\"],\n",
    "    columns=[\"Pos\", \"Neg\", \"Acc\"]\n",
    "  )\n",
    "    \n",
    "  clear_output(wait=True)\n",
    "  display(HTML(stats.to_html()))\n",
    "  print(\"Current file: \", total)\n",
    "  print(fil[:6], \":\", predicted_captcha)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Captcha-using-tensorflow-on-chars74K.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
